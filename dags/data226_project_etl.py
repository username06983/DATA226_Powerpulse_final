# -*- coding: utf-8 -*-
"""data226_project_ETL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13qg_xttL1s55mSAfkio5Ej02aOmlMRCG
"""


import pandas as pd
from airflow import DAG
from airflow.decorators import task
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from airflow.hooks.base import BaseHook
from airflow.models import Variable
#from airflow.utils.dates import days_ago
import requests

from datetime import timedelta
from datetime import datetime
import snowflake.connector

def return_snowflake_conn():
    hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')
    conn = hook.get_conn()
    return conn

BASE_URL = "https://api.eia.gov/v2/electricity/rto/region-data/data/"

@task
def extract(start: str, end: str | None = None):
    api_key = Variable.get("eia_api_key")
    params = {
        "api_key": api_key,
        "frequency": "hourly",
        "data[0]": "value",
        "start": start,
        "sort[0][column]": "period",
        "sort[0][direction]": "desc",
        "offset": 0,
        "length": 5000,
    }
    if end is not None:          # only add end when you really want a cutoff
        params["end"] = end


    all_rows = []
    while True:
        resp = requests.get(BASE_URL, params=params, timeout=60)
        resp.raise_for_status()
        rows = resp.json().get("response", {}).get("data", [])
        if not rows:
            break
        all_rows.extend(rows)
        if len(rows) < params["length"]:
            break
        params["offset"] += params["length"]

    return all_rows


@task
def transform(rows):
    if not rows:
        return []

    df = pd.DataFrame(rows)
    df["Date"] = pd.to_datetime(df["period"])
    df.rename(
        columns={
            "respondent": "Region",
            "type": "Series",
            "value": "Value",
        },
        inplace=True,
    )

    df = df[["Region", "Date", "Series", "Value"]]

    # make Date serializable for XCom
    df["Date"] = df["Date"].astype(str)

    return df.to_dict("records")


@task
def load(records: list):

    if not records:
        return

    conn = return_snowflake_conn()
    extras = (BaseHook.get_connection("snowflake_conn").extra_dejson or {})
    wh = extras.get("warehouse")
    db = extras.get("database")

    try:
        with conn.cursor() as cur:
            if wh:
                cur.execute(f'USE WAREHOUSE {wh}')
            if db:
                cur.execute(f'USE DATABASE {db}')

            schema_name = "RAW"
            target = f'{db}.{schema_name}.RTO_REGION_HOURLY' if db else f'{schema_name}.RTO_REGION_HOURLY'

            # create schema + table once
            cur.execute(f"CREATE SCHEMA IF NOT EXISTS {db}.{schema_name}")
            cur.execute(f"""
                CREATE TABLE IF NOT EXISTS {target} (
                    region VARCHAR NOT NULL,
                    date   TIMESTAMP_NTZ NOT NULL,
                    series VARCHAR NOT NULL,
                    value  FLOAT,
                    PRIMARY KEY (region, date, series)
                );
            """)
            conn.commit()

            # start transaction for this batch
            cur.execute("BEGIN;")

            # temp table for this runâ€™s rows
            cur.execute("""
                CREATE OR REPLACE TEMP TABLE RAW_BATCH_RTO_REGION_HOURLY (
                    region VARCHAR NOT NULL,
                    date   TIMESTAMP_NTZ NOT NULL,
                    series VARCHAR NOT NULL,
                    value  FLOAT
                );
            """)

            insert_tmp_sql = """
                INSERT INTO RAW_BATCH_RTO_REGION_HOURLY (region, date, series, value)
                VALUES (%s, TO_TIMESTAMP(%s), %s, %s)
            """

            data = [
                (
                    str(row["Region"]),
                    str(row["Date"]),
                    str(row["Series"]),
                    float(row["Value"]) if row["Value"] is not None else None,
                )
                for row in records
            ]
            
            batch_size = 50000  # or 100000, but < 200000
            for i in range(0, len(data), batch_size):
                batch = data[i : i + batch_size]
                cur.executemany(insert_tmp_sql, batch)

            # upsert from temp into RAW (idempotent)
            cur.execute(f"""
                MERGE INTO {target} AS tgt
                USING RAW_BATCH_RTO_REGION_HOURLY AS src
                ON  tgt.region = src.region
                AND tgt.date   = src.date
                AND tgt.series = src.series
                WHEN MATCHED THEN UPDATE SET
                    value = src.value
                WHEN NOT MATCHED THEN INSERT (region, date, series, value)
                VALUES (src.region, src.date, src.series, src.value);
            """)

            cur.execute("COMMIT;")

    except Exception:
        cur.execute("ROLLBACK;")
        raise
    finally:
        conn.close()

        
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="eia_ETL",
    default_args=default_args,
    start_date=datetime(2025, 8, 1),
    schedule="@daily",  
    catchup=False,
    tags=["eia", "snowflake", "etl"],
) as dag:

    end_date = datetime.utcnow().replace(minute=0, second=0, microsecond=0)
    start_date = end_date - timedelta(days=70)

    rows = extract(
        start=start_date.strftime("%Y-%m-%dT%H"),
        end=end_date.strftime("%Y-%m-%dT%H"),
    )


    records = transform(rows)
    load(records)
